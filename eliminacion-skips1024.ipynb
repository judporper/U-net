{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import ImageReadMode\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DATASET\n",
    "import os\n",
    "\n",
    "# read dataset for u-net\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.files = [os.path.splitext(f)[0] for f in os.listdir(data_dir)]\n",
    "        self.transform = T.Resize((512, 512))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.files[idx]\n",
    "        # read image\n",
    "        image = read_image(os.path.join(self.data_dir, file_name + \".png\"), mode=ImageReadMode.RGB)\n",
    "\n",
    "        # read mask\n",
    "        mask = read_image(os.path.join(self.label_dir, file_name + \".png\"), mode=ImageReadMode.GRAY)\n",
    "\n",
    "        # resize\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        image = image / 255.0\n",
    "        mask /= 255.0\n",
    "        mask = torch.round(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "\n",
    "ds = Dataset(\"./test\", \"./labels\")\n",
    "ds_loader = DataLoader(ds, batch_size=4)"
   ],
   "id": "645abf19d967faad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\" Convolutional block:\n",
    "    It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
    "\"\"\"\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\" Encoder block:\n",
    "    It consists of an conv_block followed by a max pooling.\n",
    "    Here the number of filters doubles and the height and width half after every block.\n",
    "\"\"\"\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "\n",
    "\"\"\" Decoder block:\n",
    "    The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
    "    connection from the encoder block. Next comes the conv_block.\n",
    "    Here the number filters decreases by half and the height and width doubles.\n",
    "\"\"\"\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs = torch.randn((2, 3, 512, 512)).to(device)\n",
    "model = build_unet().to(device)\n",
    "y = model(inputs)\n",
    "print(y.shape)"
   ],
   "id": "de243d0fc6d76ac2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3,\n",
    "                             weight_decay=1e-5)"
   ],
   "id": "453d33b5df747532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (images, labels) in enumerate(ds_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Lote:\", idx)\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ],
   "id": "896f5adc39520142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "it = iter(ds_loader)\n",
    "img, label = next(it)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "ret = model(img.to(device))\n",
    "plt.imshow(ret[1][0].detach().cpu().numpy())"
   ],
   "id": "8e7973527a163a2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Threshold the output\n",
    "ret = torch.round(ret)\n",
    "\n",
    "# Show a matrix of images of 4x2\n",
    "for i in range(4):\n",
    "    # Size image\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.subplot(4, 2, 2*i+1)\n",
    "    plt.imshow(img[i].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    plt.subplot(4, 2, 2*i+2)\n",
    "    plt.imshow(ret[i][0].detach().cpu().numpy())\n"
   ],
   "id": "eaac9da51fea59dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"model_unet_coins.pth\")",
   "id": "77337749f51e48c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#model = build_unet()\n",
    "model.load_state_dict(torch.load(\"model_unet_coins.pth\"))"
   ],
   "id": "5a227c537110e5a2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
